import logging
from stacks.data.logger import setup_logger
from stacks.data.pyspark.etl import EtlSession

WORKLOAD_NAME = "{{ pipeline_name }}"

logger_library = "stacks.data"
logger = logging.getLogger(logger_library)


def etl_main() -> None:
    """Execute data processing and transformation jobs."""
    logger.info(f"Running {WORKLOAD_NAME} processing...")

    etl_session = EtlSession(WORKLOAD_NAME)
    spark_session = etl_session.spark_session
    adls_client = etl_session.adls_client

    #######################
    # Add processing here #
    #######################

    logger.info(f"Finished: {WORKLOAD_NAME} processing.")


if __name__ == "__main__":
    setup_logger(name=logger_library, log_level=logging.INFO)
    etl_main()
